{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RishabhSheoran/CS5446/blob/main/Paper2/Paper_2_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Deep Q Networks\n",
        "\n",
        "<!-- **Before doing this, be sure you review the `README.md` file from the homework!** -->\n",
        "\n",
        "<!-- **HUGE NOTE**: the third deliverable asks you to run Pong for several thousand steps.\n",
        "Please start early, as the evaluation portion of this assignment can take many hours.\n",
        "We provide some timing benchmarks for machines in the third deliverable. -->\n",
        "\n",
        "In this part of the assignment, you will implement the Deep Q-Network algorithm\n",
        "(DQN) [1], along with the Double DQN extension [2]. \n",
        "<!-- At a high level, the `dqn.py`\n",
        "code builds a TensorFlow computational graph in the class initialization method.\n",
        "Then, to train, it iterates through environment steps and model updates. -->\n",
        "\n",
        "## Review\n",
        "\n",
        "Recall from lecture that the DQN algorithm performs the following optimization:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\rm minimize}_{\\theta} \\;\\; \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim D}\n",
        "\\left[ \n",
        "\\Big(r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a) - Q_\\theta(s_t,a_t)\\Big)^2\n",
        "\\right]\n",
        "\\end{equation}\n",
        "\n",
        "Here, $(s_t,a_t,r_t,s_{t+1})$ are batches of samples from the experience replay\n",
        "buffer $D$, which is designed to store the past $N$ samples (where usually\n",
        "$N=1,000,000$ for Atari benchmarks) so as to break correlation among the\n",
        "training data used for updating $\\theta$. In addition, we use $\\theta$ to\n",
        "represent the *current* or *online* network, whereas $\\theta^-$\n",
        "represents the *target* network. Both networks use the same architecture,\n",
        "and we use $Q_\\theta(s,a)$ or $Q_{\\theta^-}(s,a)$ to denote which of the two\n",
        "parameters is being applied to evaluate the state-action tuple $(s,a)$.\n",
        "\n",
        "The target network starts off by getting matched to the current network, but\n",
        "remains frozen (usually for thousands of steps) before getting updated again to\n",
        "match the network. The process repeats throughout training, with the goal of\n",
        "increasing the stability of the targets $r_t + \\gamma \\max_{a \\in \\mathcal{A}}\n",
        "Q_{\\theta^-}(s_{t+1},a)$. For more details, we recommend reading [1].\n",
        "\n",
        "In *Double* DQN, the target value changes slightly. The new optimization\n",
        "problem is:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\rm minimize}_{\\theta} \\;\\; \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim D}\n",
        "\\left[ \n",
        "\\Big(r_t + \\gamma Q_{\\theta^-} \\Big( s_{t+1}, {\\rm argmax}_{a\\in \\mathcal{A}} Q_\\theta(s_{t+1},a) \\Big)  - Q_\\theta(s_t,a_t)\\Big)^2\n",
        "\\right]\n",
        "\\end{equation}\n",
        "\n",
        "Notice the key difference in the target terms, which we denote $Y_t^{\\rm DQN}$\n",
        "and $Y_t^{\\rm DoubleDQN}$ following the notation in [2]. Explicitly, we\n",
        "have\n",
        "\n",
        "\\begin{equation}\n",
        "Y_t^{\\rm DQN} = r_t + \\gamma \\max_{a \\in \\mathcal{A}} Q_{\\theta^-}(s_{t+1},a)\n",
        "\\end{equation}\n",
        "\n",
        "in DQN, and\n",
        "\n",
        "\\begin{equation}\n",
        "Y_t^{\\rm DoubleDQN} = r_t + \\gamma\n",
        "Q_{\\theta^-} \\Big( s_{t+1}, {\\rm argmax}_{a\\in \\mathcal{A}} Q_\\theta(s_{t+1},a) \\Big)\n",
        "\\end{equation}\n",
        "\n",
        "in DDQN. Intuitively, DDQN helps to mitigate the issue of over-optimistic values\n",
        "from DQN. In DQN, for a given state $s$, the Q-network takes the maximum over\n",
        "quantities $Q_{\\theta^-}(s,a_k)$ for all actions $a_k \\in \\mathcal{A}$. In DDQN,\n",
        "we consider the same $Q_{\\theta^-}(s,a_k)$ values, yet are not guaranteed to\n",
        "take the maximum one because the action selection is done by a different\n",
        "network.  For more details, we recommend reading [2]. The paper also\n",
        "covers the Double Q-Learning algorithm, which was originally developed in the\n",
        "tabular settings.  The Double *Deep Q-Network* algorithm, which you need to implement in this assignment, is the minimal extension of DQN towards Double Q-Learning.\n",
        "\n",
        "# Noted that you need to implement Double DQN in this assignment.\n",
        "\n",
        "[1]: [DQN](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf), read the Nature 2015 paper, not the NIPS 2013 workshop paper\n",
        "\n",
        "[2]: [DDQN](https://arxiv.org/abs/1509.06461) at AAAI, 2016.\n"
      ],
      "metadata": {
        "id": "_MMdRkdHwxrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DDQN Implementation\n",
        "\n",
        "\n",
        "**You will need to write code in `DQN.ipynb` file in three specific spots.\n",
        "These are clearly indicated with the comments `START OF YOUR CODE` and\n",
        "`END OF YOUR CODE`.** Put your code between those comments. Concretely, they correspond to:\n",
        "\n",
        "- Environment Stepping\n",
        "- Training by computing the Bellman Error.\n",
        "\n",
        "**See the comments in `DQN.ipynb`, which provide detailed instructions. You\n",
        "should not need to modify any other files for this part of the assigment**. \n",
        "\n",
        "Some advice: implementing DQN can be tricky, and it is sometimes difficult to know if your algorithm is working immediately due to the many samples necessary for deep reinforcement learning. Thus, we advice:\n",
        "\n",
        "- Do not engage in (extensive) hyperparmeter tuning. We provide\n",
        "hyperparameters for you in `DQN.ipynb` for CartPole environment that\n",
        "have empirically worked for our solutions. If you have the time, you may find it\n",
        "helpful to tune the learning rate for CartPole."
      ],
      "metadata": {
        "id": "J3mut5X7ymLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Considerations\n",
        "\n",
        "As in the first part on policy gradients, you should test your code by running Jupyter notebook cells. Make sure that you don't change the cells, as the graders need consistency in seeing the results."
      ],
      "metadata": {
        "id": "NFlunNCqwxXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMeV606OgsDH",
        "outputId": "64156159-115b-4275-966d-cd58db7eea1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.10.5\n",
            "  Downloading gym-0.10.5.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=bd45023d95c58a43eba31964e57023845ecc02e456b392075b1e869b5659101a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.5\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.10.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e1xhY1yvZWAB"
      },
      "outputs": [],
      "source": [
        "import random, time, gym, sys\n",
        "import gym.spaces\n",
        "from gym.wrappers.monitor import Monitor\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from collections import namedtuple\n",
        "import pickle\n",
        "import time\n",
        "import uuid\n",
        "import os\n",
        "import sys\n",
        "from collections import deque\n",
        "\n",
        "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Jv6WGqkAfC"
      },
      "source": [
        "## Set Up Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNDYo5uqUxJr",
        "outputId": "622c71d0-32dc-40b7-e453-7d648963a952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random seed = 3\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        }
      ],
      "source": [
        "env='CartPole-v0'\n",
        "seed=3\n",
        "num_steps=5e4\n",
        "double_q=True\n",
        "\n",
        "if seed is None:\n",
        "    seed = random.randint(0, 9999)\n",
        "print('random seed = {}'.format(seed))\n",
        "exp_name = 'double-dqn'\n",
        "\n",
        "logdir = exp_name+ '_' +env+ '_' +time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join('data_dqn', logdir)\n",
        "\n",
        "def set_global_seeds(i):\n",
        "    np.random.seed(i)\n",
        "    random.seed(i)\n",
        "\n",
        "env = gym.make(env)\n",
        "set_global_seeds(seed)\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "expt_dir = os.path.join(logdir, \"gym\")\n",
        "env = Monitor(env, expt_dir, force=True, video_callable=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "        self.dense3 = nn.Linear(32, num_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = F.tanh(self.dense2(x))\n",
        "        out = self.dense3(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "js2DALaqtp2f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIyQdLRFkGxj"
      },
      "source": [
        "## Set Up Exploration Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OXoWTuTySRwt"
      },
      "outputs": [],
      "source": [
        "def linear_interpolation(l, r, alpha):\n",
        "    return l + alpha * (r - l)\n",
        "\n",
        "class PiecewiseSchedule(object):\n",
        "\n",
        "    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n",
        "        \"\"\"Piecewise schedule.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        endpoints: [(int, int)]\n",
        "            list of pairs `(time, value)` meanining that schedule should output\n",
        "            `value` when `t==time`. All the values for time must be sorted in\n",
        "            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n",
        "            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n",
        "            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n",
        "            time passed between `time_a` and `time_b` for time `t`.\n",
        "        interpolation: lambda float, float, float: float\n",
        "            a function that takes value to the left and to the right of t according\n",
        "            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n",
        "            right endpoint that t has covered. See linear_interpolation for example.\n",
        "        outside_value: float\n",
        "            if the value is requested outside of all the intervals sepecified in\n",
        "            `endpoints` this value is returned. If None then AssertionError is\n",
        "            raised when outside value is requested.\n",
        "        \"\"\"\n",
        "        idxes = [e[0] for e in endpoints]\n",
        "        assert idxes == sorted(idxes)\n",
        "        self._interpolation = interpolation\n",
        "        self._outside_value = outside_value\n",
        "        self._endpoints      = endpoints\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n",
        "            if l_t <= t and t < r_t:\n",
        "                alpha = float(t - l_t) / (r_t - l_t)\n",
        "                return self._interpolation(l, r, alpha)\n",
        "\n",
        "        # t does not belong to any of the pieces, so doom.\n",
        "        assert self._outside_value is not None\n",
        "        return self._outside_value\n",
        "\n",
        "exploration_schedule = PiecewiseSchedule([\n",
        "        (0,   1.00),\n",
        "        (5e3, 0.03),\n",
        "        (1e4, 0.01),\n",
        "    ], outside_value=0.01\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbUcHigekaAh"
      },
      "source": [
        "## Set Up Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "14k1aDmnSpku"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# REPLAY BUFFER\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class BasicBuffer:\n",
        "\n",
        "  def __init__(self, max_size):\n",
        "      self.max_size = max_size\n",
        "      self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "      experience = (state, action, np.array([reward]), next_state, done)\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      state_batch = []\n",
        "      action_batch = []\n",
        "      reward_batch = []\n",
        "      next_state_batch = []\n",
        "      done_batch = []\n",
        "\n",
        "      batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "      for experience in batch:\n",
        "          state, action, reward, next_state, done = experience\n",
        "          state_batch.append(state)\n",
        "          action_batch.append(action)\n",
        "          reward_batch.append(reward)\n",
        "          next_state_batch.append(next_state)\n",
        "          done_batch.append(done)\n",
        "\n",
        "      return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.buffer)\n",
        "\n",
        "def get_wrapper_by_name(env, classname):\n",
        "    currentenv = env\n",
        "    while True:\n",
        "        if classname in currentenv.__class__.__name__:\n",
        "            return currentenv\n",
        "        elif isinstance(env, gym.Wrapper):\n",
        "            currentenv = currentenv.env\n",
        "        else:\n",
        "            raise ValueError(\"Couldn't find wrapper named %s\"%classname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvP8UR7kskG"
      },
      "source": [
        "## Run Q-Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([1])\n",
        "a.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPAI3oijNADC",
        "outputId": "d2c5b149-8199-447f-a5b2-575a6271e671"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hqNzlrOIR4dh"
      },
      "outputs": [],
      "source": [
        "class QLearner(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 q_func,\n",
        "                 t_func,\n",
        "                 optimizer_spec,\n",
        "                 exploration,\n",
        "                 replay_buffer_size,\n",
        "                 batch_size,\n",
        "                 gamma,\n",
        "                 learning_starts,\n",
        "                 target_update_freq,\n",
        "                 grad_norm_clipping,\n",
        "                 double_q=True,\n",
        "                 max_steps=2e8,\n",
        "                 cartpole=True):\n",
        "        \"\"\"Run Deep Q-learning algorithm.\n",
        "\n",
        "        You can specify your own convnet using `q_func`.\n",
        "        All schedules are w.r.t. total number of steps taken in the environment.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: gym.Env\n",
        "            gym environment to train on.\n",
        "        q_func: function\n",
        "            Model to use for computing the q function. \n",
        "        t_func: function\n",
        "            Model to use for computing the target q function. \n",
        "        optimizer_spec: OptimizerSpec\n",
        "            Specifying the constructor and kwargs, as well as learning rate schedule\n",
        "            for the optimizer\n",
        "        exploration: \n",
        "            exploration schedule\n",
        "        replay_buffer_size: int\n",
        "            How many memories to store in the replay buffer.\n",
        "        batch_size: int\n",
        "            How many transitions to sample each time experience is replayed.\n",
        "        gamma: float\n",
        "            Discount Factor\n",
        "        learning_starts: int\n",
        "            After how many environment steps to start replaying experiences\n",
        "        target_update_freq: int\n",
        "            How many experience replay rounds (not steps!) to perform between\n",
        "            each update to the target Q network\n",
        "        grad_norm_clipping: float or None\n",
        "            If not None gradients' norms are clipped to this value.\n",
        "        double_q: bool\n",
        "            If True, use double Q-learning to compute target values. Otherwise, vanilla DQN.\n",
        "            https://papers.nips.cc/paper/3964-double-q-learning.pdf\n",
        "        max_steps: int\n",
        "            Maximum number of training steps. The number of *frames* is 4x this\n",
        "            quantity (modulo the initial random no-op steps).\n",
        "        cartpole: bool\n",
        "            If True, CartPole-v0.\n",
        "        \"\"\"\n",
        "        assert type(env.observation_space) == gym.spaces.Box\n",
        "        assert type(env.action_space)      == gym.spaces.Discrete\n",
        "        self.q_func = q_func\n",
        "        self.t_func = t_func\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.optimizer = optimizer_spec\n",
        "        self.exploration = exploration\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.learning_starts = learning_starts\n",
        "        self.double_q = double_q\n",
        "        self.cartpole = cartpole\n",
        "        self.env = env\n",
        "        self.replay_buffer = BasicBuffer(max_size=replay_buffer_size)\n",
        "\n",
        "        self.input_shape = self.env.observation_space.shape # should be (4,)\n",
        "        self.num_actions = self.env.action_space.n\n",
        "\n",
        "        self.num_param_updates = 0\n",
        "        self.mean_episode_reward      = -float('nan')\n",
        "        self.std_episode_reward       = -float('nan')\n",
        "        self.best_mean_episode_reward = -float('inf') \n",
        "        self.steps = []\n",
        "        self.mean_rewards = []\n",
        "        self.log_every_n_steps = 1000\n",
        "        self.start_time = time.time()\n",
        "        self.last_obs = self.env.reset()\n",
        "        self.t = 0\n",
        "\n",
        "\n",
        "    def step_env(self):\n",
        "        \"\"\"Step the env and store the transition.\n",
        "\n",
        "        At this point, `self.last_obs` contains the latest observation that was\n",
        "        recorded from the simulator. Here, your code\n",
        "        needs to store this observation and outcome (reward, next observation,\n",
        "        etc.) into the replay buffer while doing one step in the env simulator.\n",
        "\n",
        "        At the end of this block of code, the simulator should have been\n",
        "        advanced one step, the replay buffer should contain one more transition,\n",
        "        and, `self.last_obs` must point to the new latest observation.\n",
        "\n",
        "        Useful functions you'll need to call:\n",
        "\n",
        "            self.env.step(action)\n",
        "            self.replay_buffer.push(self.last_obs, action, reward, next_obs, done)\n",
        "\n",
        "        This steps the environment forward one step. And:\n",
        "\n",
        "            obs = self.env.reset()\n",
        "\n",
        "        This resets the environment if you reached an episode boundary.  Call\n",
        "        `self.env.reset()` to get a new observation if `done=True`. For CartPole, \n",
        "        this is guaranteed to start a new episode as they don't have a\n",
        "        notion of 'ale.lives' in them.\n",
        "\n",
        "        Don't forget to include epsilon greedy exploration!  \n",
        "        \"\"\"\n",
        "        # ----------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Hint: Don't forget to include epsilon greedy exploration!\n",
        "        # if random.random() < self.exploration.value(self.t):\n",
        "        #   ...\n",
        "        # else:\n",
        "        #   ...\n",
        "        # ...\n",
        "        # next_obs, _, _, _ = self.env.step(action)\n",
        "        # Hint: Do not forget to update `self.last_obs` based on next observation (next_obs)\n",
        "        # Hint: Call `self.env.reset()` to get a new observation and start a new episode if `done=True`.\n",
        "        \n",
        "        rate = self.exploration.value(self.t)\n",
        "        if rate > random.random():\n",
        "            # exploration\n",
        "            action =  random.randrange(self.num_actions) \n",
        "        else:\n",
        "            # exploitation\n",
        "            with torch.no_grad():\n",
        "                action = self.q_func(torch.from_numpy(self.last_obs).float().to(device)).argmax().item()   \n",
        "        # print('action: ', action)\n",
        "        action = torch.LongTensor([action]).to(device)\n",
        "        next_obs, reward, done, _ = self.env.step(action.item())\n",
        "\n",
        "        self.replay_buffer.push(self.last_obs, action, reward, next_obs, done)\n",
        "        self.last_obs = next_obs\n",
        "\n",
        "        if done:\n",
        "            self.last_obs = self.env.reset()\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "    def update_model(self):\n",
        "        \"\"\"Perform experience replay and train the network.\n",
        "\n",
        "        This is only done if the replay buffer contains enough samples for us to\n",
        "        learn something useful -- until then, the model will not be initialized\n",
        "        and random actions should be taken.  Training consists of four steps:\n",
        "        \n",
        "        3.a: Use the replay buffer to sample a batch of transitions. See the\n",
        "        replay buffer code for function definitions. You may use thi function:\n",
        "\n",
        "          self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        3.b: Compute the total Bellman error in a batch. You can use MSE loss to\n",
        "        achieve it.\n",
        "        \n",
        "        3.c: Perform a gradient step and update the network parameters to reduce\n",
        "        total_error. \n",
        "        \n",
        "        3.d: Periodically update the target network (self.t_func) every \n",
        "        `self.target_update_freq` steps.\n",
        "\n",
        "        Hint: You can also use `param.grad.data.clamp` to clip gradient, where \n",
        "        `param in self.q_func.parameters()`.\n",
        "        \"\"\"\n",
        "        if (self.t > self.learning_starts and \\\n",
        "            len(self.replay_buffer) > self.batch_size):\n",
        "\n",
        "            # ------------------------------------------------------------------\n",
        "            # START OF YOUR CODE\n",
        "            # ------------------------------------------------------------------\n",
        "            #  \n",
        "\n",
        "            state_batch, actions_tensor_list, reward_batch, next_state_batch, done_batch = self.replay_buffer.sample(self.batch_size)\n",
        "            \n",
        "            action_batch = []\n",
        "            for t in actions_tensor_list:\n",
        "                action_batch.append(t.item())\n",
        "            action_batch = torch.from_numpy(np.array(action_batch)).long().to(device)\n",
        "            state_batch  = torch.from_numpy(np.array(state_batch)).float().to(device)\n",
        "            reward_batch = torch.from_numpy(np.array(reward_batch)).float().to(device)\n",
        "            next_state_batch = torch.from_numpy(np.array(next_state_batch)).float().to(device) \n",
        "            done_batch = torch.from_numpy(np.array(done_batch)).long().to(device)\n",
        "\n",
        "            current_q_values = self.q_func(state_batch).gather(dim=1, index=action_batch.unsqueeze(-1))\n",
        "            inner_action = self.q_func(next_state_batch).argmax(dim=1).detach()\n",
        "            target_q_values = reward_batch + self.gamma * self.t_func(next_state_batch).gather(dim=1, index=inner_action.unsqueeze(-1)) * (1-done_batch).unsqueeze(-1)\n",
        "\n",
        "            loss = F.mse_loss(current_q_values, target_q_values)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            for param in self.q_func.parameters():\n",
        "                param.grad.data.clamp\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.t % self.target_update_freq == 0:\n",
        "                self.t_func.load_state_dict(self.q_func.state_dict())\n",
        "\n",
        "            # ------------------------------------------------------------------\n",
        "            # END OF YOUR CODE\n",
        "            # ------------------------------------------------------------------\n",
        "            self.num_param_updates += 1\n",
        "        self.t += 1\n",
        "\n",
        "    def log_progress(self):\n",
        "        episode_rewards = get_wrapper_by_name(self.env, \"Monitor\").get_episode_rewards()\n",
        "\n",
        "        if len(episode_rewards) > 0:\n",
        "            self.mean_episode_reward = np.mean(episode_rewards[-10:])\n",
        "            self.std_episode_reward  = np.std(episode_rewards[-10:])\n",
        "        if len(episode_rewards) > 10:\n",
        "            self.best_mean_episode_reward = \\\n",
        "                max(self.best_mean_episode_reward, self.mean_episode_reward)\n",
        "\n",
        "        if self.t % self.log_every_n_steps == 0:\n",
        "            self.steps.append(self.t)\n",
        "            self.mean_rewards.append(self.mean_episode_reward)\n",
        "            hours = (time.time() - self.start_time) / (60.*60.)\n",
        "            print(\"Steps: \",                 self.t)\n",
        "            print(\"Avg_Last_10_Episodes\", self.mean_episode_reward)\n",
        "            print(\"Std_Last_10_Episodes\", self.std_episode_reward)\n",
        "            print(\"Best_Avg_10_Episodes\", self.best_mean_episode_reward)\n",
        "            print(\"Num_Episodes\",          len(episode_rewards))\n",
        "            print(\"Exploration_Epsilon\",   self.exploration.value(self.t))\n",
        "            print(\"Elapsed_Time_Hours\",    hours)\n",
        "\n",
        "    def checkSolveEnv(self):\n",
        "        if self.mean_episode_reward > 195:\n",
        "            return 1, self.t\n",
        "        return 0, self.t\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn_learn(*args, **kwargs):\n",
        "    alg = QLearner(*args, **kwargs)\n",
        "    flag = 0\n",
        "    while True:\n",
        "        # \n",
        "        alg.step_env()\n",
        "        alg.update_model()\n",
        "        alg.log_progress()\n",
        "        solved, steps = alg.checkSolveEnv()\n",
        "        if solved and not flag:\n",
        "            print('Solved the environment after {} steps'.format(steps))\n",
        "            flag = 1\n",
        "            # break\n",
        "        if alg.t > alg.max_steps:\n",
        "            print(\"\\nt = {} exceeds max_steps = {}\".format(alg.t, alg.max_steps))\n",
        "            break\n",
        "    return alg.steps, alg.mean_rewards"
      ],
      "metadata": {
        "id": "t6xQNm0d6mVu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(4, n_actions).to(device)\n",
        "target_net = DQN(4, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=4e-4)\n",
        "\n",
        "steps, mean_rewards = dqn_learn(\n",
        "    env=env,\n",
        "    q_func=policy_net,\n",
        "    t_func=target_net,\n",
        "    optimizer_spec=optimizer,\n",
        "    exploration=exploration_schedule,\n",
        "    replay_buffer_size=50000,\n",
        "    batch_size=100,\n",
        "    gamma=0.99,\n",
        "    learning_starts=1000,\n",
        "    target_update_freq=100,\n",
        "    grad_norm_clipping=10,\n",
        "    double_q=double_q,\n",
        "    max_steps=num_steps,\n",
        "    cartpole=True\n",
        "    )\n",
        "\n",
        "# env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmaQDyOotkHF",
        "outputId": "8adc6244-92b6-47b5-ef4f-4fc44ef26e56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps:  1000\n",
            "Avg_Last_10_Episodes 27.7\n",
            "Std_Last_10_Episodes 14.269197594819408\n",
            "Best_Avg_10_Episodes 28.1\n",
            "Num_Episodes 42\n",
            "Exploration_Epsilon 0.806\n",
            "Elapsed_Time_Hours 7.982843452029759e-05\n",
            "Steps:  2000\n",
            "Avg_Last_10_Episodes 17.9\n",
            "Std_Last_10_Episodes 6.7149087856798175\n",
            "Best_Avg_10_Episodes 29.1\n",
            "Num_Episodes 92\n",
            "Exploration_Epsilon 0.612\n",
            "Elapsed_Time_Hours 0.0010721213287777371\n",
            "Steps:  3000\n",
            "Avg_Last_10_Episodes 63.9\n",
            "Std_Last_10_Episodes 36.30275471641236\n",
            "Best_Avg_10_Episodes 63.9\n",
            "Num_Episodes 113\n",
            "Exploration_Epsilon 0.41800000000000004\n",
            "Elapsed_Time_Hours 0.0020682905779944526\n",
            "Steps:  4000\n",
            "Avg_Last_10_Episodes 130.9\n",
            "Std_Last_10_Episodes 50.442938058761015\n",
            "Best_Avg_10_Episodes 130.9\n",
            "Num_Episodes 120\n",
            "Exploration_Epsilon 0.22399999999999998\n",
            "Elapsed_Time_Hours 0.003078074786398146\n",
            "Steps:  5000\n",
            "Avg_Last_10_Episodes 155.2\n",
            "Std_Last_10_Episodes 19.635681806344284\n",
            "Best_Avg_10_Episodes 158.1\n",
            "Num_Episodes 126\n",
            "Exploration_Epsilon 0.03\n",
            "Elapsed_Time_Hours 0.00410292731391059\n",
            "Steps:  6000\n",
            "Avg_Last_10_Episodes 152.4\n",
            "Std_Last_10_Episodes 11.79152237838694\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 133\n",
            "Exploration_Epsilon 0.026\n",
            "Elapsed_Time_Hours 0.005149914953443739\n",
            "Steps:  7000\n",
            "Avg_Last_10_Episodes 144.6\n",
            "Std_Last_10_Episodes 8.333066662399863\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 140\n",
            "Exploration_Epsilon 0.022\n",
            "Elapsed_Time_Hours 0.006186836825476752\n",
            "Steps:  8000\n",
            "Avg_Last_10_Episodes 145.5\n",
            "Std_Last_10_Episodes 7.864477096412704\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 146\n",
            "Exploration_Epsilon 0.018000000000000002\n",
            "Elapsed_Time_Hours 0.00722186803817749\n",
            "Steps:  9000\n",
            "Avg_Last_10_Episodes 143.7\n",
            "Std_Last_10_Episodes 8.307225770376053\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 153\n",
            "Exploration_Epsilon 0.014000000000000002\n",
            "Elapsed_Time_Hours 0.008264199958907233\n",
            "Steps:  10000\n",
            "Avg_Last_10_Episodes 143.5\n",
            "Std_Last_10_Episodes 6.08687111741328\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 161\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.0093126466539171\n",
            "Steps:  11000\n",
            "Avg_Last_10_Episodes 138.3\n",
            "Std_Last_10_Episodes 4.775981574503821\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 168\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.010364693072107103\n",
            "Steps:  12000\n",
            "Avg_Last_10_Episodes 140.2\n",
            "Std_Last_10_Episodes 7.586830695356263\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 175\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.011413487394650777\n",
            "Steps:  13000\n",
            "Avg_Last_10_Episodes 142.4\n",
            "Std_Last_10_Episodes 8.534635317340747\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 182\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.012466215358840094\n",
            "Steps:  14000\n",
            "Avg_Last_10_Episodes 140.2\n",
            "Std_Last_10_Episodes 5.793099343184094\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 189\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.013518421649932862\n",
            "Steps:  15000\n",
            "Avg_Last_10_Episodes 137.2\n",
            "Std_Last_10_Episodes 5.344155686354955\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 196\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.014576366742451986\n",
            "Steps:  16000\n",
            "Avg_Last_10_Episodes 140.3\n",
            "Std_Last_10_Episodes 7.668767828015137\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 203\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.015629069407780964\n",
            "Steps:  17000\n",
            "Avg_Last_10_Episodes 148.0\n",
            "Std_Last_10_Episodes 9.348796714016196\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 210\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.01687749617629581\n",
            "Steps:  18000\n",
            "Avg_Last_10_Episodes 148.2\n",
            "Std_Last_10_Episodes 11.956588142108101\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 217\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.017945865790049236\n",
            "Steps:  19000\n",
            "Avg_Last_10_Episodes 144.9\n",
            "Std_Last_10_Episodes 11.157508682497182\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 224\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.01901004393895467\n",
            "Steps:  20000\n",
            "Avg_Last_10_Episodes 146.8\n",
            "Std_Last_10_Episodes 9.558242516278817\n",
            "Best_Avg_10_Episodes 162.8\n",
            "Num_Episodes 230\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.020071209934022692\n",
            "Steps:  21000\n",
            "Avg_Last_10_Episodes 164.0\n",
            "Std_Last_10_Episodes 15.073154945133417\n",
            "Best_Avg_10_Episodes 164.0\n",
            "Num_Episodes 236\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.021127262314160664\n",
            "Steps:  22000\n",
            "Avg_Last_10_Episodes 186.9\n",
            "Std_Last_10_Episodes 16.483021567661677\n",
            "Best_Avg_10_Episodes 186.9\n",
            "Num_Episodes 241\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.02219307667679257\n",
            "Solved the environment after 22652 steps\n",
            "Steps:  23000\n",
            "Avg_Last_10_Episodes 197.6\n",
            "Std_Last_10_Episodes 7.2\n",
            "Best_Avg_10_Episodes 197.6\n",
            "Num_Episodes 246\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.02324951145384047\n",
            "Steps:  24000\n",
            "Avg_Last_10_Episodes 197.6\n",
            "Std_Last_10_Episodes 7.2\n",
            "Best_Avg_10_Episodes 197.6\n",
            "Num_Episodes 251\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.024305360582139758\n",
            "Steps:  25000\n",
            "Avg_Last_10_Episodes 197.1\n",
            "Std_Last_10_Episodes 8.700000000000001\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 256\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.025382344391610887\n",
            "Steps:  26000\n",
            "Avg_Last_10_Episodes 194.1\n",
            "Std_Last_10_Episodes 9.689685237405806\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 262\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.026444107161627876\n",
            "Steps:  27000\n",
            "Avg_Last_10_Episodes 194.6\n",
            "Std_Last_10_Episodes 9.090654541890808\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 267\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.027512837251027426\n",
            "Steps:  28000\n",
            "Avg_Last_10_Episodes 197.4\n",
            "Std_Last_10_Episodes 7.800000000000001\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 272\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.028582906458112928\n",
            "Steps:  29000\n",
            "Avg_Last_10_Episodes 199.0\n",
            "Std_Last_10_Episodes 3.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 277\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.029640725321239896\n",
            "Steps:  30000\n",
            "Avg_Last_10_Episodes 199.0\n",
            "Std_Last_10_Episodes 3.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 282\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.030709925029012892\n",
            "Steps:  31000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 287\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03178118083212111\n",
            "Steps:  32000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 292\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03284465856022305\n",
            "Steps:  33000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 297\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03391845438215468\n",
            "Steps:  34000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 302\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.034999007715119254\n",
            "Steps:  35000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 307\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.036077609592013886\n",
            "Steps:  36000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 312\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03714953945742713\n",
            "Steps:  37000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 317\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03822945084836748\n",
            "Steps:  38000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 322\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.03933566947778066\n",
            "Steps:  39000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 327\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.040426318844159446\n",
            "Steps:  40000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 332\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.041630261805322435\n",
            "Steps:  41000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 337\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.04271771066718631\n",
            "Steps:  42000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 342\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.04380133416917589\n",
            "Steps:  43000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 347\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.04487651209036509\n",
            "Steps:  44000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 352\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.045952834089597064\n",
            "Steps:  45000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 357\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.04703333417574564\n",
            "Steps:  46000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 362\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.048141245312160914\n",
            "Steps:  47000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 367\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.04922040104866028\n",
            "Steps:  48000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 372\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.050301395522223576\n",
            "Steps:  49000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 377\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.05138405700524648\n",
            "Steps:  50000\n",
            "Avg_Last_10_Episodes 200.0\n",
            "Std_Last_10_Episodes 0.0\n",
            "Best_Avg_10_Episodes 200.0\n",
            "Num_Episodes 382\n",
            "Exploration_Epsilon 0.01\n",
            "Elapsed_Time_Hours 0.05247598833507962\n",
            "\n",
            "t = 50001 exceeds max_steps = 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(steps, mean_rewards)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Return\")"
      ],
      "metadata": {
        "id": "xptQv4SXu4cL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "aa37dc4d-dfe7-435a-b4ba-11aafafca035"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Return')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(steps, mean_rewards)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Return\")"
      ],
      "metadata": {
        "id": "mpfh0lFX2wiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "7a50967b-8033-42b6-d25a-a85ccbaea6e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Return')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DDQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}